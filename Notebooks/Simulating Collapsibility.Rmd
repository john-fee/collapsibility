---
title: "Simulating collapsibility"
author: "John Fee"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.height = 7,
  fig.width = 10,
  fig.align = "center"
  )
```

```{r}
# I/O
library(here)

# Data manipulation
library(dplyr)
library(purrr)
library(tidyr)

# Graphics
library(ggplot2)
library(ggdag)
library(dagitty)
library(latex2exp)
theme_set(cowplot::theme_cowplot())

# Misc
source(here("R","sim-functions.R"))
set.seed(1)
```

We're going to walk through a few different scenarios with different data generating processes, and via simulation examine how well we can recover the true value of the parameter of interest.

# A linear model

## Scenario 1 - No confounders

We measure the values of a continuous response variable $Y$, a binary treatment variable $X$, and measured covariates $W$ and $Z$.  $X$, $Y$, and $Z$ are mutually independent, but $Y$ is dependent upon all of them.  We can represent this dependency structure graphically in the form of a DAG.

```{r,fig.height=4,fig.width=4}
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z
) %>%
  ggdag() +
  theme_void()
```

To simulate data compatible with this structure, I draw the predictors from a multivariate normal distribution with a diagonal covariance matrix (i.e. they are independent draws from separate normal distributions).  $X$ is thresholded to convert it into a binary variable, and $Y$ is generated by the equation

$$
\begin{aligned}
  Y &= X + 3W + 1.5Z + \varepsilon\\
  \varepsilon &\sim \mathcal{N}(0,9)
\end{aligned}
$$


This process is repeated until 1000 datasets of size $n = 500$ are generated. I then fit the same linear model to each simulated dataset, and plot the distribution of the results and provide a table of summary stats.
```{r cache = TRUE}
cov_matrix_diagonal <- matrix(
  c(
    4,0,0,
    0,4,0,
    0,0,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  y_formula = 3*W + X + 1.5*Z,
  n_simulations = 1000,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z + W,data = .x))
    )
```

```{r}
coef_df <- data.frame(
  parameter = c("(Intercept)","X","Z","W"),
  true_value = c(0,1,1.5,3)
  ) %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from the same linear model"
  )
```



```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data")
```

As expected, the linear model provides unbiased estimates of the effect associated with our treatment variable $X$.  Varying the dataset size or the amount of noise (variance of $\varepsilon$) would change the shape of the simulated parameter distributions, but they would remain unbiased. What if we have a more complicated dependency structure?

## Scenario 2 - Confounders present

Let's take the same setup as scenario 1, but instead $X$ and $Y$ both depend on $Z$.  

```{r,fig.height=4,fig.width=4}
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z,
  X ~ Z
) %>%
  ggdag() +
  theme_void()
```

It is sufficient to condition on $X$ and $Z$ to recover the correct coefficient for $X$.  Let's simulate the data and model fitting and check that this this true.

```{r cache = TRUE}
cov_matrix_diagonal <- matrix(
  c(
    4,0,0,
    0,4,3,
    0,3,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  y_formula = 3*W + X + 1.5*Z,
  n_simulations = 1000,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z + W,data = .x))
    )
```

```{r}
coef_df <- data.frame(
  parameter = c("(Intercept)","X","Z","W"),
  true_value = c(0,1,1.5,3)
  ) %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from the same linear model"
  )
```

```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data with confounder")
```

The above simulation matches with our supposition that conditioning on $Z$ will provide unbiased estimates of the coefficient for $X$.  Note that the variance of the estimate for $X$ is larger than in scenario 1, when $X$ and $Z$ were uncorrelated (this is expected).  Now what happens if we remove $W$, a prognostic variable, from the model we fit, but *not* the DGP?

## Scenario 3 - Confounders present, prognostic variable omitted

```{r cache = TRUE}
cov_matrix_diagonal <- matrix(
  c(
    4,0,0,
    0,4,3,
    0,3,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  y_formula = 3*W + X + 1.5*Z,
  n_simulations = 1000,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z,data = .x))
    )
```

```{r}
coef_df <- data.frame(
  parameter = c("(Intercept)","X","Z","W"),
  true_value = c(0,1,1.5,3)
  ) %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from a model missing a prognostic variable"
  ) +
  lims(x = c(-2.5,5))
```

```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data with confounder present and prognostic variable omitted")
```

Coefficient for $X$ remains unbiased, even though prognostic variable $W$ is removed.  This is a property exclusive to regression with identity and log link functions, as we shall see in the following sections.

# A (non identity/log link) GLM - logistic regression



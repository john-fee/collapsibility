---
title: "Simulating collapsibility"
author: "John Fee"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.height = 7,
  fig.width = 10,
  fig.align = "center"
  )
```

```{r}
# I/O
library(here)

# Data manipulation
library(dplyr)
library(purrr)
library(tidyr)

# Graphics
library(ggplot2)

library(ggdag)
library(dagitty)
library(latex2exp)
theme_set(cowplot::theme_cowplot())

# Misc
source(here("R","sim-functions.R"))
set.seed(1)
n_simulations = 2000
```

I'm going to walk through a few different scenarios with different data generating processes, and via simulation examine how well I can recover the true value of the parameter of interest.

# A linear model

## Scenario 1 - No confounders

For our DGP, assume that I observe the values of a continuous response variable $Y$, a binary treatment variable $X$, and measured covariates $W$ and $Z$.  $X$, $Y$, and $Z$ are mutually independent, but $Y$ is dependent upon all of them.  This dependency structure can be represented graphically in the form of a DAG.

```{r,fig.height=4,fig.width=4}
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z
) %>%
  ggdag() +
  theme_void()
```

To simulate data compatible with this structure, I draw the predictors from a multivariate normal distribution with a diagonal covariance matrix (i.e. they are independent draws from separate normal distributions).  $X$ is thresholded to convert it into a binary variable, and $Y$ is generated by the following equations

$$
\begin{aligned}
  Y &= X + 3W + 1.5Z + \varepsilon\\
  \varepsilon &\sim \mathcal{N}(0,9)
\end{aligned}
$$

This process is repeated until 1000 datasets of size `r scales::label_comma()(n_simulations)` are generated. I then fit the same linear model to each simulated dataset, and plot the distribution of the results and provide a table of summary stats.

```{r cache = TRUE}
cov_matrix_diagonal <- matrix(
  c(
    4,0,0,
    0,4,0,
    0,0,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  n_simulations = n_simulations,
  simulate_function = simulate_linear_model,
  y_formula = 3*W + X + 1.5*Z,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z + W,data = .x))
    )
```

```{r}
true_coefs <- data.frame(
  parameter = c("(Intercept)","X","Z","W"),
  true_value = c(0,1,1.5,3)
  )

coef_df <- true_coefs %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from the same linear model"
  )
```



```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data")
```

As expected, the linear model provides unbiased estimates of the effect associated with our treatment variable $X$.  Varying the dataset size or the amount of noise (variance of $\varepsilon$) would change the shape of the simulated parameter distributions, but they would remain unbiased. What if we have a more complicated dependency structure?

## Scenario 2 - Confounders present

Let's take the same setup as scenario 1, but instead $X$ and $Y$ both depend on $Z$.  

```{r,fig.height=4,fig.width=4}
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z,
  X ~ Z
) %>%
  ggdag() +
  theme_void()
```

It is sufficient to condition on $X$ and $Z$ to recover the correct coefficient for $X$.  Let's simulate the data and model fitting and check that this this true.

```{r cache = TRUE}
cov_matrix_confounding <- matrix(
  c(
    4,0,0,
    0,4,3,
    0,3,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_confounding,
  n_simulations = n_simulations,
  simulate_function = simulate_linear_model,
  y_formula = 3*W + X + 1.5*Z,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z + W,data = .x))
    )
```

```{r}
coef_df <- true_coefs %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from the same linear model"
  )
```

```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data with confounder")
```

The above simulation matches with our supposition that conditioning on $Z$ will provide unbiased estimates of the coefficient for $X$.  Note that the variance of the estimate for $X$ is larger than in scenario 1, when $X$ and $Z$ were uncorrelated (this is expected).  Now what happens if we remove $W$, a prognostic variable, from the models I fit, but *do not*  remove $W$ from the DGP?

## Scenario 3 - Confounders present, prognostic variable omitted

I now repeat scenario 3, but remove $W$ from the models I fit to the data.

```{r cache = TRUE}
results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_confounding,
  n_simulations = n_simulations,
  simulate_function = simulate_linear_model,
  y_formula = 3*W + X + 1.5*Z,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z,data = .x))
    )
```

```{r}
coef_df <- true_coefs %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from a model missing a prognostic variable"
  ) +
  lims(x = c(-2.5,5))
```

```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data with confounder present and prognostic variable omitted")
```

Coefficient for $X$ remains unbiased, even though prognostic variable $W$ is removed.  This is a property exclusive to regression with identity and log link functions, as we shall see in the following sections.

# A (non identity/log link) GLM - logistic regression

## Scenario 1 - No confounders

For this section, I'm going to modify our DGP to be compatible with a different type of model. Assume that I observe the values of a **discrete** response variable $Y$, a binary treatment variable $X$, and measured covariates $W$ and $Z$.  $X$, $Y$, and $Z$ are mutually independent, but $Y$ is dependent upon all of them.
 
The DAG is the same as in scenario 1 for linear regression (since it's just a graph of dependencies between variables).

```{r,fig.height=4,fig.width=4}
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z
) %>%
  ggdag() +
  theme_void()
```

To simulate data compatible with this structure and the logistic regression model, I draw the predictors from a multivariate normal distribution with a diagonal covariance matrix (i.e. they are independent draws from separate normal distributions).  $X$ is thresholded to convert it into a binary variable, and $Y$ is generated by the equations below.

$$
\begin{aligned}
  A &= X + 3W + 1.5Z\\
  Y_{prob} &= \frac{1}{1 + \exp{A}}\\
  Y &\sim \text{Bernoulli}(Y_{prob})
\end{aligned}
$$
 
```{r cache = TRUE}
results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  n_simulations = n_simulations,
  simulate_function = simulate_logistic_reg_model,
  y_formula = 3*W + X + 1.5*Z,
  x_threshold = qnorm(0.5) # Chosen so X is a 50/50 split in both treatment groups
) %>%
  mutate(
    fitted_model = map(data,~ glm(
      outcome ~ X + Z + W,
      family = binomial(link = "logit"),
      data = .x))
    )
``` 
 
 
```{r}
coef_df <- true_coefs %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of logistic regression parameter estimates \nfit on data simulated from the same model"
  )
```

```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ logistic GLMs fit on simulated data")
```

Parameter estimates for logistic regression are biased (upwards) in finite samples, so we expect some positive bias.  This isn't too bad (we could do better with e.g. Firth penalized logistic regression), but we're mainly interested in differences between the scenarios.

## Scenario 2 - Confounders present 

Let's take the same setup as scenario 1, but instead $X$ and $Y$ both depend on $Z$.  

```{r,fig.height=4,fig.width=4}
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z,
  X ~ Z
) %>%
  ggdag() +
  theme_void()
```

```{r cache = TRUE}
results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_confounding,
  n_simulations = n_simulations,
  simulate_function = simulate_logistic_reg_model,
  y_formula = 3*W + X + 1.5*Z,
  x_threshold = qnorm(0.5) # Chosen so X is a 50/50 split in both treatment groups
) %>%
  mutate(
    fitted_model = map(data,~ glm(
      outcome ~ X + Z + W,
      family = binomial(link = "logit"),
      data = .x))
    )
``` 
 
 
```{r}
coef_df <- true_coefs %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of logistic regression parameter estimates \nfit on data simulated from the same model"
  )
```


```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ logistic GLMs fit on simulated data with confounders")
```

Basically the same results - no surprises there (we adjusted for the confounder).

## Scenario 3 - Confounders present, prognostic variable omitted

Now I remove $W$ from the model fitting.

```{r cache = TRUE}
results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_confounding,
  n_simulations = n_simulations,
  simulate_function = simulate_logistic_reg_model,
  y_formula = 3*W + X + 1.5*Z,
  x_threshold = qnorm(0.5) # Chosen so X is a 50/50 split in both treatment groups
) %>%
  mutate(
    fitted_model = map(data,~ glm(
      outcome ~ X + Z,
      family = binomial(link = "logit"),
      data = .x))
    )
``` 
 
 
```{r}
coef_df <- true_coefs %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of logistic regression parameter estimates \nfit on data simulated from the same model"
  )
```


```{r}
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ logistic GLMs fit on simulated data with confounders and prognostic variable omitted.")
```

The estimated coefficients are now way off, even though we conditioned on the only confounder!  The non-collapsibility of logistic regression implies that we need to condition on all variables that contribute to variation in $Y$.

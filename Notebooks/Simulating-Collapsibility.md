Simulating collapsibility
================
John Fee
2024-02-27

``` r
# I/O
library(here)

# Data manipulation
library(dplyr)
library(purrr)
library(tidyr)

# Graphics
library(ggplot2)
library(ggdag)
library(dagitty)
library(latex2exp)
theme_set(cowplot::theme_cowplot())

# Misc
source(here("R","sim-functions.R"))
set.seed(1)
```

We’re going to walk through a few different scenarios with different
data generating processes, and via simulation examine how well we can
recover the true value of the parameter of interest.

# A linear model

## Scenario 1 - No confounders

We measure the values of a continuous response variable $Y$, a binary
treatment variable $X$, and measured covariates $W$ and $Z$. $X$, $Y$,
and $Z$ are mutually independent, but $Y$ is dependent upon all of them.
We can represent this dependency structure graphically in the form of a
DAG.

``` r
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z
) %>%
  ggdag() +
  theme_void()
```

<img src="Simulating-Collapsibility_files/figure-gfm/unnamed-chunk-2-1.png" style="display: block; margin: auto;" />

To simulate data compatible with this structure, I draw the predictors
from a multivariate normal distribution with a diagonal covariance
matrix (i.e. they are independent draws from separate normal
distributions). $X$ is thresholded to convert it into a binary variable,
and $Y$ is generated by the equation

$$
\begin{aligned}
  Y &= X + 3W + 1.5Z + \varepsilon\\
  \varepsilon &\sim \mathcal{N}(0,9)
\end{aligned}
$$

This process is repeated until 1000 datasets of size $n = 500$ are
generated. I then fit the same linear model to each simulated dataset,
and plot the distribution of the results and provide a table of summary
stats.

``` r
cov_matrix_diagonal <- matrix(
  c(
    4,0,0,
    0,4,0,
    0,0,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  y_formula = 3*W + X + 1.5*Z,
  n_simulations = 1000,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z + W,data = .x))
    )
```

``` r
coef_df <- data.frame(
  parameter = c("(Intercept)","X","Z","W"),
  true_value = c(0,1,1.5,3)
  ) %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from the same linear model"
  )
```

<img src="Simulating-Collapsibility_files/figure-gfm/unnamed-chunk-4-1.png" style="display: block; margin: auto;" />

``` r
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data")
```

| Variable    | Parameter estimate | True value |       Bias |
|:------------|-------------------:|-----------:|-----------:|
| (Intercept) |          0.0023046 |        0.0 |  0.0023046 |
| W           |          2.9840192 |        3.0 | -0.0159808 |
| X           |          1.0034573 |        1.0 |  0.0034573 |
| Z           |          1.4959494 |        1.5 | -0.0040506 |

Summary table for parameters of $n = 1000$ linear models fit on
simulated data

As expected, the linear model provides unbiased estimates of the effect
associated with our treatment variable $X$. Varying the dataset size or
the amount of noise (variance of $\varepsilon$) would change the shape
of the simulated parameter distributions, but they would remain
unbiased. What if we have a more complicated dependency structure?

## Scenario 2 - Confounders present

Let’s take the same setup as scenario 1, but instead $X$ and $Y$ both
depend on $Z$.

``` r
dagify(
  Y ~ X,
  Y ~ W,
  Y ~ Z,
  X ~ Z
) %>%
  ggdag() +
  theme_void()
```

<img src="Simulating-Collapsibility_files/figure-gfm/unnamed-chunk-6-1.png" style="display: block; margin: auto;" />

It is sufficient to condition on $X$ and $Z$ to recover the correct
coefficient for $X$. Let’s simulate the data and model fitting and check
that this this true.

``` r
cov_matrix_diagonal <- matrix(
  c(
    4,0,0,
    0,4,3,
    0,3,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  y_formula = 3*W + X + 1.5*Z,
  n_simulations = 1000,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z + W,data = .x))
    )
```

``` r
coef_df <- data.frame(
  parameter = c("(Intercept)","X","Z","W"),
  true_value = c(0,1,1.5,3)
  ) %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from the same linear model"
  )
```

<img src="Simulating-Collapsibility_files/figure-gfm/unnamed-chunk-8-1.png" style="display: block; margin: auto;" />

``` r
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data with confounder")
```

| Variable    | Parameter estimate | True value |       Bias |
|:------------|-------------------:|-----------:|-----------:|
| (Intercept) |          0.0333366 |        0.0 |  0.0333366 |
| W           |          2.9993847 |        3.0 | -0.0006153 |
| X           |          0.9296940 |        1.0 | -0.0703060 |
| Z           |          1.5169205 |        1.5 |  0.0169205 |

Summary table for parameters of $n = 1000$ linear models fit on
simulated data with confounder

The above simulation matches with our supposition that conditioning on
$Z$ will provide unbiased estimates of the coefficient for $X$. Note
that the variance of the estimate for $X$ is larger than in scenario 1,
when $X$ and $Z$ were uncorrelated (this is expected). Now what happens
if we remove $W$, a prognostic variable, from the model we fit, but
*not* the DGP?

## Scenario 3 - Confounders present, prognostic variable omitted

``` r
cov_matrix_diagonal <- matrix(
  c(
    4,0,0,
    0,4,3,
    0,3,4
  ),
  nrow = 3,
  ncol = 3,
  byrow = TRUE
)

results <- simulate_multiple_datasets(
  n = 500,
  sigma = cov_matrix_diagonal,
  y_formula = 3*W + X + 1.5*Z,
  n_simulations = 1000,
  x_threshold = qnorm(0.5), # Chosen so X is a 50/50 split in both treatment groups
  epsilon_sd = 9
) %>%
  mutate(
      fitted_model = map(data,~ lm(Y ~ X + Z,data = .x))
    )
```

``` r
coef_df <- data.frame(
  parameter = c("(Intercept)","X","Z","W"),
  true_value = c(0,1,1.5,3)
  ) %>%
  get_coef_df(results,.)

plot_coefficient_distribution(
  coef_df,
  title = "Distribution of linear regression parameter estimates \nfit on data simulated from a model missing a prognostic variable"
  ) +
  lims(x = c(-2.5,5))
```

<img src="Simulating-Collapsibility_files/figure-gfm/unnamed-chunk-11-1.png" style="display: block; margin: auto;" />

``` r
coef_df %>%
  summarize_coef_df(caption = "Summary table for parameters of $n = 1000$ linear models fit on simulated data with confounder present and prognostic variable omitted")
```

| Variable    | Parameter estimate | True value |       Bias |
|:------------|-------------------:|-----------:|-----------:|
| (Intercept) |         -0.0118844 |        0.0 | -0.0118844 |
| X           |          1.0254737 |        1.0 |  0.0254737 |
| Z           |          1.4841530 |        1.5 | -0.0158470 |

Summary table for parameters of $n = 1000$ linear models fit on
simulated data with confounder present and prognostic variable omitted

Coefficient for $X$ remains unbiased, even though prognostic variable
$W$ is removed. This is a property exclusive to regression with identity
and log link functions, as we shall see in the following sections.

# A (non identity/log link) GLM - logistic regression
